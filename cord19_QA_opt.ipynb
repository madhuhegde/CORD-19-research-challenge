{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 17:34:30.109962 4493012288 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "from nltk import tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name(author):\n",
    "    middle_name = \" \".join(author['middle'])\n",
    "    \n",
    "    if author['middle']:\n",
    "        return \" \".join([author['first'], middle_name, author['last']])\n",
    "    else:\n",
    "        return \" \".join([author['first'], author['last']])\n",
    "\n",
    "\n",
    "def format_affiliation(affiliation):\n",
    "    text = []\n",
    "    location = affiliation.get('location')\n",
    "    if location:\n",
    "        text.extend(list(affiliation['location'].values()))\n",
    "    \n",
    "    institution = affiliation.get('institution')\n",
    "    if institution:\n",
    "        text = [institution] + text\n",
    "    return \", \".join(text)\n",
    "\n",
    "def format_authors(authors, with_affiliation=False):\n",
    "    name_ls = []\n",
    "    \n",
    "    for author in authors:\n",
    "        name = format_name(author)\n",
    "        if with_affiliation:\n",
    "            affiliation = format_affiliation(author['affiliation'])\n",
    "            if affiliation:\n",
    "                name_ls.append(f\"{name} ({affiliation})\")\n",
    "            else:\n",
    "                name_ls.append(name)\n",
    "        else:\n",
    "            name_ls.append(name)\n",
    "    \n",
    "    return \", \".join(name_ls)\n",
    "\n",
    "def format_body(body_text):\n",
    "    texts = [(di['section'], di['text']) for di in body_text]\n",
    "    texts_di = {di['section']: \"\" for di in body_text}\n",
    "    \n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "\n",
    "    for section, text in texts_di.items():\n",
    "        body += section\n",
    "        body += \"\\n\\n\"\n",
    "        body += text\n",
    "        body += \"\\n\\n\"\n",
    "    \n",
    "    return body\n",
    "\n",
    "def format_bib(bibs):\n",
    "    if type(bibs) == dict:\n",
    "        bibs = list(bibs.values())\n",
    "    bibs = deepcopy(bibs)\n",
    "    formatted = []\n",
    "    \n",
    "    for bib in bibs:\n",
    "        bib['authors'] = format_authors(\n",
    "            bib['authors'], \n",
    "            with_affiliation=False\n",
    "        )\n",
    "        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n",
    "        formatted.append(\", \".join(formatted_ls))\n",
    "\n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "\n",
    "def format_body_text(body_text):\n",
    "    \n",
    "    body = \"\"\n",
    "\n",
    "    for di in body_text:\n",
    "        text = di['text']\n",
    "        body += text\n",
    "    return body\n",
    "    \n",
    "    \n",
    "def format_corpus_text(body_text, min_len=18, max_len=128):\n",
    "    junk_text = \"copyright\"\n",
    "    \n",
    "    def remove_braces_brackets(body_text):\n",
    "        body_text = re.sub(r'\\([0-9]+\\)', '', body_text)\n",
    "        body_text = re.sub(r'\\[[^)]*\\]', '', body_text)\n",
    "        return(body_text)\n",
    "        \n",
    "    body_text = remove_braces_brackets(body_text)\n",
    "    text_lines = []\n",
    "    token_lines = tokenize.sent_tokenize(body_text)\n",
    "    for line in token_lines:\n",
    "      \n",
    "        words = line.split()\n",
    "        if junk_text not in words:\n",
    "             max_word_len = len(max(words, key=len))\n",
    "             if (len(words) > min_len) and (len(words) < max_len) and max_word_len > 5:\n",
    "                 text_lines.append(line)\n",
    "    \n",
    "    return(text_lines)\n",
    "\n",
    "def find_filenames(folder):\n",
    "    all_files = glob.glob(f'{folder}/**/*.json', recursive=True)\n",
    "    print(\"Number of articles retrieved from the folder:\", len(all_files))\n",
    "    files = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        with open(filename) as f:\n",
    "            file = json.load(open(filename))\n",
    "            files.append(file)\n",
    "    return(files) \n",
    "\n",
    "\n",
    "def find_file_index(folder):\n",
    "    all_files = glob.glob(f'{folder}/**/*.json', recursive=True)\n",
    "    path_name = []\n",
    "    path_dict = {}\n",
    "    path_dict_inv = {}\n",
    "    file_index = []\n",
    "\n",
    "\n",
    "    for filename in all_files:\n",
    "        last = filename.split('/')[-1]\n",
    "        first = filename.replace(last, '')\n",
    "        #print(first)\n",
    "        #print(last)\n",
    "        if first not in path_name:\n",
    "            path_name.append(first)\n",
    "            path_dict[first] = len(path_name)-1\n",
    "            path_dict_inv[len(path_name)-1] = first\n",
    "        file_index.append((path_dict[first], last))   \n",
    "        \n",
    "    print(len(file_index))\n",
    "    return file_index, path_dict_inv \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_data(files):\n",
    "    cleaned_text = []\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        body_text = format_body_text(file['body_text'])\n",
    "        body_text = body_text.replace('\\n',' ')\n",
    "\n",
    "        features = [\n",
    "           file['metadata']['title'],\n",
    "           format_authors(file['metadata']['authors'], with_affiliation=True),\n",
    "           body_text]\n",
    "        cleaned_text.append(features)\n",
    "    \n",
    "    col_names = [\n",
    "       'title',\n",
    "       'authors',\n",
    "       'paragraphs']\n",
    "\n",
    "    clean_df = pd.DataFrame(cleaned_text, columns=col_names)\n",
    "    return(clean_df)\n",
    "\n",
    "\n",
    "def find_index_text(file_index, path_dict, index):\n",
    "    indexed_files = []\n",
    "    \n",
    "    for i in index:\n",
    "        filename = path_dict[file_index[i][0]]+file_index[i][1]\n",
    "\n",
    "        with open(filename) as f:\n",
    "            file = json.load(open(filename))\n",
    "            indexed_files.append(file)\n",
    "        \n",
    "    frame = generate_clean_data(indexed_files)\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BM25Retriever(BM25Okapi):\n",
    "    def __init__(self, lowercase=True, tokenizer=None, top_n=10, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        super().__init__(\"dummy\", tokenizer=None, k1=k1, b=b, epsilon=epsilon)\n",
    "        self.lowercase = lowercase\n",
    "        self.top_n = top_n\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_doc = 0\n",
    "        self.corpus_size = 0\n",
    "        self.nd = Counter({})\n",
    "        \n",
    "    def fit_retriever(self, documents):\n",
    "        doc_list = [document for document in documents]\n",
    "        #print(len(doc_list))\n",
    "        if self.tokenizer:\n",
    "            tokenized_text = [self.tokenizer(document) for document in doc_list]\n",
    "        else:\n",
    "            tokenized_text = [document.split(\" \") for document in doc_list]\n",
    "   \n",
    "        #print(tokenized_text[0])\n",
    "        self.corpus_size = self.corpus_size+len(tokenized_text)\n",
    "        num_doc = 0\n",
    "        for doc_tokens in tokenized_text:\n",
    "            num_doc += len(doc_tokens)\n",
    "        self.num_doc = self.num_doc+num_doc   \n",
    "        self.avgdl = self.num_doc/self.corpus_size\n",
    "        \n",
    "        #print(self.corpus_size, self.num_doc, self.avgdl)\n",
    "        nd = Counter(self._initialize(tokenized_text))\n",
    "        self.nd = self.nd + nd\n",
    "        #print(len(self.doc_freqs), len(self.doc_len))\n",
    "        \n",
    "    def compute_params(self):    \n",
    "        self._calc_idf(self.nd)\n",
    "        \n",
    "    def compute_scores(self, query):\n",
    "        if(self.tokenizer == None):\n",
    "           tokenized_query = query.split(\" \")\n",
    "        else:\n",
    "           tokenizer = self.tokenizer\n",
    "           tokenized_query = tokenizer(query)\n",
    "      \n",
    "        doc_scores = self.get_scores(tokenized_query)\n",
    "\n",
    "        #return top_n indices and scores as list\n",
    "        sorted_scores = np.argsort(doc_scores)\n",
    "        top_n = self.top_n\n",
    "        out = zip(sorted_scores[-1:-top_n-1:-1],doc_scores[sorted_scores[-1:-top_n-1:-1]])\n",
    "        return list(out)   \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Wordpiece tokenizer\n",
    "bert_tokenizer =  BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "retriever = BM25Retriever(tokenizer=bert_tokenizer.tokenize)\n",
    "\n",
    "sub_folders = glob.glob('./input/CORD-19-research-challenge/*/')\n",
    "for folder in sub_folders[:1]:\n",
    "    files = find_filenames(folder)\n",
    "    if(len(files) > 0):\n",
    "        frame = generate_clean_data(files)\n",
    "    retriever.fit_retriever(frame['paragraphs'])\n",
    "   \n",
    "#Compute TF-IDF paramas\n",
    "retriever.compute_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59311\n",
      "59311\n",
      "{0: './input/CORD-19-research-challenge/custom_license/custom_license/pmc_json/', 1: './input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/', 2: './input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pmc_json/', 3: './input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/', 4: './input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/', 5: './input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pmc_json/', 6: './input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'}\n"
     ]
    }
   ],
   "source": [
    "folders = './input/CORD-19-research-challenge/*/'\n",
    "\n",
    "file_index, path_dict_inv = find_file_index(folders)\n",
    "print(len(file_index))\n",
    "print(path_dict_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top_n documents based on BM250 for the query \n",
    "query = \"what is covid-19\"\n",
    "doc_scores = retriever.compute_scores(query)\n",
    "\n",
    "#Select top_n documents\n",
    "index = [score[0] for score in doc_scores]\n",
    "text = find_index_text(file_index, path_dict_inv, index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reader = DocReader('./input/model/')\n",
    "reader = DocReader('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "ans = reader.predict(df=text, query=query, n_best=5)\n",
    "b_answer = reader.best_answer(ans)\n",
    "print(b_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
